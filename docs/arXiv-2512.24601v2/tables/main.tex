\begin{table*}[ht]
\centering
\caption{Performance comparison of different methods across long-context benchmarks of varying complexity. In \textcolor{gray}{gray} is the average API cost $\pm$ the standard deviation of each method on each task. $^{*}$ indicates runs where a method (sometimes) ran into input context limits. Provider costs were computed under OpenAI for GPT-5 and Fireworks for other models. Non-zero scores are rounded to at least $0.1$.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l@{\hskip 6pt}c@{\hskip 6pt}c@{\hskip 6pt}c@{\hskip 6pt}c}
\toprule
\textbf{Model} & \textbf{CodeQA} & \textbf{BrowseComp+ (1K)} & \textbf{OOLONG} & \textbf{OOLONG-Pairs} \\
\midrule
\textbf{Task Length $N$ (tokens)} & 23K-4.2M & 6M-11M & 131K & 32K \\
\midrule
\multicolumn{5}{l}{\textbf{GPT-5} {\tiny (with RLM sub-calls to GPT-5-mini)}} \\
\midrule
Base Model     
& \score{24.0$^{*}$}{0.13}{0.07}
& \scoreNA{0.0$^{*}$}
& \score{44.0}{0.14}{0.02}
& \score{0.1}{0.16}{0.10} \\ %

CodeAct (+ BM25)      
& \score{22.0$^{*}$}{0.06}{0.08}
& \score{51.0}{0.71}{1.20}
& \score{38.0}{0.61}{1.06}
& \score{24.7}{0.75}{0.43} \\

CodeAct (+ sub-calls)   
& \score{24.0$^{*}$}{0.06}{0.08}
& \scoreNA{0.0$^{*}$}
& \score{40.0}{0.85}{1.27}
& \score{28.4}{1.11}{0.62} \\

Summary agent     
& \score{58.0}{1.31}{1.46}
& \score{70.5}{0.57}{0.10}
& \score{46.0}{0.13}{0.01}
& \score{0.1}{0.13}{0.09} \\

RLM         
& \score{\textbf{62.0}}{0.11}{0.10}
& \score{\textbf{91.3}}{0.99}{1.22}
& \score{\textbf{56.5}}{0.43}{0.85}
& \score{\textbf{58.0}}{0.33}{0.20} \\

RLM (no sub-calls)   
& \score{58.0}{0.18}{0.56}
& \score{88.0}{0.44}{0.90}
& \score{36.0}{0.37}{0.42}
& \score{43.9}{0.69}{1.16} \\
\midrule
\multicolumn{5}{l}{\textbf{Qwen3-Coder-480B-A35B}} \\
\midrule
Base Model     
& \score{20.0$^{*}$}{0.13}{0.08}
& \scoreNA{0.0$^{*}$}
& \score{36.0}{0.06}{0.00}
& \score{0.1}{0.05}{0.01} \\

CodeAct (+ BM25)      
& \score{24.0$^{*}$}{0.17}{0.08}
& \score{12.7}{0.39}{0.50}
& \score{38.0}{1.51}{1.09}
& \score{0.3}{1.54}{0.35} \\

CodeAct (+ sub-calls)
& \score{26.0$^{*}$}{0.28}{0.30}
& \scoreNA{0.0$^{*}$}
& \score{32.0}{1.83}{1.14}
& \score{0.1}{1.49}{0.46} \\

Summary agent     
& \score{50.0}{1.26}{1.50}
& \score{38.0}{8.98}{2.12}
& \score{44.1}{0.15}{0.01}
& \score{0.31}{0.05}{0.00} \\

RLM         
& \score{56.0}{0.92}{1.23}
& \score{44.7}{0.84}{0.63}
& \score{\textbf{48.0}}{0.61}{0.49}
& \score{\textbf{23.1}}{1.02}{0.52} \\

RLM (no sub-calls)   
& \score{\textbf{66.0}}{0.18}{0.58}
& \score{\textbf{46.0}}{0.82}{0.69}
& \score{43.5}{0.32}{0.13}
& \score{17.3}{1.77}{1.23} \\

\midrule
\multicolumn{5}{l}{\textbf{Qwen3-8B}} \\
\midrule
Base Model     
& \score{4.0$^{*}$}{0.01}{0.00}
& \scoreNA{0.0$^{*}$}
& \scoreNA{0.0$^{*}$}
& \score{0.1}{0.01}{0.00} \\

RLM      
& \score{26.0}{0.04}{0.13}
& \score{2.0}{0.03}{0.06}
& \score{24.0}{0.19}{0.26}
& \score{4.3}{0.05}{0.05} \\

RLM (fine-tuned)   
& \score{\textbf{32.0}}{0.02}{0.02}
& \score{\textbf{14.0}}{0.01}{0.03}
& \score{\textbf{32.0}}{0.04}{0.09}
& \score{\textbf{5.2}}{0.02}{0.02} \\



\bottomrule
\end{tabular}}
\label{tab:main}
\end{table*}

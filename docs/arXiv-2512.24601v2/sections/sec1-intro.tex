\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.43\textwidth]{figures/scaling_plot_vertical_flip.pdf}
    \caption{A comparison of GPT-5 and a corresponding \RLM{} using GPT-5 on three long-context tasks of increasing complexity: \textbf{S-NIAH}, \textbf{OOLONG}, and \textbf{OOLONG-Pairs}. For each task, we scale the input length from $2^{13}$ to $2^{18}$. GPT-5 performance degrades significantly as a function of both input length and task complexity, while the \RLM{} maintains strong performance.
    Inputs beyond the red region do not fit in GPT-5's context window of 272K tokens, but the \RLM{} handles them effectively. Additional experiments across other models and benchmarks are in \S\ref{sec4:long-input}.
    }
    \vspace{-3mm}
    \label{fig:rlm-scaling}
\end{figure}

Frontier reasoning models have limited context windows and, even within their limits, tend to exhibit \textit{context rot}~\citep{hong2025contextrot}, a phenomenon illustrated in Figure~\ref{fig:rlm-scaling} where quality degrades steeply as prompts get longer. Though we expect context lengths to steadily rise through improvements to training, architecture, and infrastructure, we are interested in \textit{whether it is possible to scale the context size of general-purpose LLMs by orders of magnitude}. This is increasingly urgent as LLMs begin to be widely adopted for long-horizon tasks, in which they must routinely process tens if not hundreds of millions of tokens.

We study this question through the lens of scaling inference-time compute. We are inspired by the way that \textit{reasoning models} have become the fundamental interface to LLMs, resulting not only in empirical gains but also additional theoretical expressive power~\cite{merrillexpressive} compared to vanilla Transformers. %
Though most inference-time methods for dealing with long context are task-specific~\citep{wu2021recursivelysummarizingbookshuman,chang2024booookscore}, the most popular general approach is \textit{context condensation} or \textit{compaction} \citep{khattab2021baleen,smith2025openhands_context_condensensation,openai_codex_cli,wu2025resumunlockinglonghorizonsearch}, where context from user requests or agent trajectories is repeatedly summarized once it exceeds a length threshold. Unfortunately, compaction is rarely expressive enough for tasks that require dense access throughout the prompt. It presumes that \textit{some} details that appear early in the prompt can safely be forgotten to make room for new content.


\begin{figure*}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/Fig2.png}
    \caption{A Recursive Language Model (\RLM{}) treats prompts as part of the environment. It loads the input prompt as a variable inside a REPL environment $\mathcal{E}$ and writes code to peek into, decompose, and invoke itself recursively over programmatic snippets of the variable.}
    \label{fig:rlm-repl}
    \vspace{-1em}
\end{figure*}

We introduce \textbf{Recursive Language Models} (\textbf{\RLM{}}s), a general-purpose inference paradigm for dramatically scaling the effective input and output lengths of LLMs. The key insight is that arbitrarily long user prompts should not be fed into the neural network (e.g., Transformer) directly but should instead be treated as \textit{part of the environment that the LLM is tasked to \textbf{symbolically and recursively} interact with}.

As Figure~\ref{fig:rlm-repl} shows, an RLM exposes the same external interface as an LLM or a reasoning model: it accepts a string prompt of arbitrary structure and produces a string response. Given a prompt $P$, the RLM initializes a Read-Eval-Print Loop (REPL) programming environment in which $P$ is set as the value of a variable. It then offers the LLM general context about the REPL environment (e.g., the length of the string $P$), and permits it to write code that peeks into and decomposes $P$, and to iteratively observe any side effects from execution. Crucially, RLMs encourage the LLM to understand, transform, and execute the input prompt by \textit{writing symbolic programs that invoke the LLM itself} on as many slices of the input as necessary.

By treating the prompt itself as an external object and enabling symbolic recursion, RLMs tackle limitations of expressive power in recent work on coding agents, retrieval agents, and sub-agent delegation. In particular, prior coding agents and retrieval agents treat some designated external data source (e.g., a filesystem or a corpus of search documents) as an environment for fetching snippets. However, they \textit{can only fill up the underlying LLM's context window with snippets before breaking down}. Similarly, prior self-delegation approaches~\citep{anthropic_claude_code_subagents,sentient2025roma,schroeder2025threadthinkingdeeperrecursive, sun2025scalinglonghorizonllmagent} allow LLMs to invoke themselves as sub-agents. However, they \textit{are handicapped by the underlying LLM's limited output lengths} because they are designed to verbalize sub-calls autoregressively rather than producing them programmatically. %


We evaluate \RLM{}s using a frontier closed model (GPT-5;~\citealt{singh2025openaigpt5card}) and a frontier open model (Qwen3-Coder-480B-A35B;~\citealt{Qwen3-Coder-480B-A35B}) across four tasks with varying levels of complexity: deep research~\citep{chen2025browsecompplusfairtransparentevaluation}, information aggregation~\citep{bertsch2025oolongevaluatinglongcontext}, code repository understanding~\citep{bai2025longbenchv2deeperunderstanding}, and a synthetic pairwise reasoning task where even frontier models fail catastrophically. We compare RLMs against direct LLM calls as well as context compaction, retrieval tool-use agents, and code-generation agents.

We find that RLMs demonstrate extremely strong performance even at the 10M+ token scale, and substantially outperform all other approaches at long-context processing, in many cases by double-digit percentage gains while maintaining comparable cost. In particular, as demonstrated in Figure~\ref{fig:rlm-scaling}, \RLM{}s exhibit far less severe degradation for longer contexts and more sophisticated tasks. 

Finally, at a small scale, we post-train the first natively recursive language model, demonstrating that \RLM{}s can be improved quickly with little additional training. While a small open model (Qwen3-8B;~\citealt{yang2025qwen3technicalreport}) struggles to solve long context tasks even in an RLM scaffold, our simple general-purpose training recipe uses only 1,000 samples from unrelated domains to improve its performance by a median of $28.3\%$ across the four evaluation tasks.

Given a base neural language model $\mathcal{M}$ with maximum context size $K$, a Recursive Language Model (\RLM{}) is an inference-time scaffold around $\mathcal{M}$ that treats the user prompt as part of the environment without giving up the ability to densely process its content through different calls to $\mathcal{M}$.
Given an arbitrary-length prompt string $P\in\Sigma^\star$, an \RLM{} interacts with a persistent external environment $\mathcal{E}$ and returns a response string $Y\in\Sigma^\star$ (Figure~\ref{fig:rlm-repl}).
We would like effectively \emph{unbounded input tokens} ($|P|\gg K$), \emph{unbounded output tokens}, and an \emph{unbounded semantic horizon}, e.g. the ability to do $\Omega(|P|)$ or $\Omega(|P|^2)$ semantic work.

Algorithm~\ref{alg:rlm-call} describes how an RLM achieves this. Given a prompt $P$, the RLM initializes a persistent REPL programming environment with a variable containing the user prompt as a string and a function for invoking a sub-RLM with a new prompt. Then, it starts the RLM loop. In the first iteration, the algorithm invokes the \textit{root} neural model $\mathcal{M}$ with only (constant-size) metadata about the user prompt, like its length, a short prefix, and how to access parts of it.

The root is instructed via prompting (Appendix~\ref{appx3:methods}) and/or fine-tuning (Appendix~\ref{appx5:training}) to operate like an RLM: that is, to \textit{generate code that helps it understand and transform its parts of its prompt $P$}, and to build up intermediate values and the final response into new variables, potentially by \textit{invoking the sub-RLM within loops}. In Section~\ref{sec4.3-results}, we find that existing LLMs can be prompted to do this and that training an 8B model to be natively recursive is promising.

Each iteration of the RLM loop executes code in the REPL, updates REPL state (intermediate variables), and collects in \texttt{stdout} any printed text. Only (constant-size) metadata about \texttt{stdout}, like a short prefix and length, is appended to $\mathcal{M}$'s history for the next iteration.\footnote{This is key: it forces $\mathcal{M}$ to rely on variables and sub-calls to manage long strings instead of polluting its window. In principle, if we trim each turn to $c$ tokens, we will have at most $K/c$ root iterations, each of which can launch arbitrarily many sub-calls. This is not a fundamental limitation, e.g. one could move the root horizon itself into a variable, but we typically want to limit the iterations at any level of recursion irrespective.} Once the RLM sets the variable \texttt{Final} inside the REPL, iteration stops and the value in \texttt{Final} is returned as the response.

RLMs make three simple design choices that are missing from existing scaffolds. To highlight these, we include Algorithm~\ref{alg:bad-agent} to illustrate a deceptively ``similar'' algorithm that is far less expressive. Both algorithms support some notion of sub-calls, external objects, and code execution, but they differ in terms of where the prompt and intermediate values live and where recursion occurs.

\input{tables/pseudocode}
\input{tables/bad-algorithm}


First, an RLM must give the underlying LLM $\mathcal{M}$ a \emph{symbolic handle} to the user prompt $P$, so the model can manipulate it without copying text into the root context window. Instead, ineffective Algorithm~\ref{alg:bad-agent} starts by putting the user prompt $P$ into the LLM context window (\texttt{hist}) and thus inherits the window limitations of $\mathcal{M}$ and falls back to heuristics like context compaction. Even though the scaffold can access external data with, say, a \texttt{Search} action or filesystem access, it is fatally bounded with respect to user input.

Second, ineffective Algorithm~\ref{alg:bad-agent} asks $\mathcal{M}$ to autoregressively generate the output directly, via a \texttt{Finish} action. This may seem innocuous, but it means that it also cannot generate longer outputs than the context window of $\mathcal{M}$ permits.

Third, and perhaps most importantly, an RLM requires \emph{symbolic recursion}. That is, code running \emph{inside} $\mathcal{E}$ must be able to invoke $\mathcal{M}$ on programmatically constructed transformations of $P$ (e.g., inside arbitrarily large loops), storing intermediate results symbolically. Though Algorithm~\ref{alg:bad-agent} includes both a code execution action and a ``sub-LLM'' action separately, it is not able to invoke the sub-LLM programmatically and hence can only delegate a few \textit{explicitly verbalized tasks} rather than writing short programs that can, say, loop over slices of the prompt and launch $\Omega(|P|)$ or even $\Omega(|P|^2)$ processes to understand or transform all parts of $P$.

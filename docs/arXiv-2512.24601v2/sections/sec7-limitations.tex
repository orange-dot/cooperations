While \RLM{}s show strong performance on tasks beyond the context window limitations of existing LMs at reasonable inference costs, evaluations for more difficult and natural long-context processing tasks and the best mechanisms for implementing \RLM{}s both remain highly under-explored. We focused on synchronous sub-calls inside of a Python REPL environment, but we note that alternative strategies involving asynchronous sub-calls and sandboxed REPLs can potentially significantly reduce the runtime and inference cost of \RLM{}s. Furthermore, we chose to use a max recursion depth of one (i.e. sub-calls are LMs); while we found strong performance on existing long-context benchmarks, we believe that future work should investigate deeper levels of recursion or even new hybrids between symbolic recursion and neural attention. We include additional limitations and negative results in Appendix~\ref{appx0:didnt-work}.

Lastly, we focused our experiments on evaluating \RLM{}s using \textit{existing} frontier models, but show initial evidence on a Qwen3-8B model that explicitly training a model to be used as a \RLM{} provides very rapid performance improvements, even outside the training domain. We hypothesize that \RLM{} trajectories can be viewed as a form of reasoning~\citep{openai2024openaio1card,deepseekai2025deepseekr1incentivizingreasoningcapability}, which can be trained by bootstrapping existing models~\citep{zelikman2022starbootstrappingreasoningreasoning, zelikman2024quietstarlanguagemodelsteach}. We hope that training native \RLM{}s can be treated as a new axis of scale to improve LM performance on general and long-horizon tasks.

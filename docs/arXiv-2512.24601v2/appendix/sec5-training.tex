

We trained \textbf{RLM-Qwen3-8B} as a very small scale exercise in training the first natively recursive language model. We hypothesized that, though acting as an RLM appears to produce sophisticated behavior due to recursion, it can be sufficient to focus on improving the root LM's ability to interact with the programmatic representation of the prompt in the REPL and to discern when sub-calls are useful.
In other words, while a typical RLM trajectory can be extremely long due to all of the sub-calls potentially launched (possibly $\Omega(|P|)$ for a prompt $P$), the leaf sub-calls are essentially general-purpose LLM requests and the major hurdle is learning to operate as the root model.

This simple insight allowed us to explore a similarly simple recipe for training. In particular, we sampled \RLM{} trajectories from a larger language model (Qwen3-Coder-480B-A35B-Instruct;~\citealt{Qwen3-Coder-480B-A35B}) and, after filtering, distilled them to a smaller model (Qwen3-8B;~\citealt{Qwen3-8B}) from the same model family.
We evaluated \RLM(Qwen3-Coder-480B-A35B) on 750 English LongBenchPro~\cite{chen2026longbenchprorealisticcomprehensive} tasks, collecting a total of 2250 candidate trajectories. 

We first remove trajectories that score exactly 0.0 on the benchmark or do not go beyond one turn, bringing it down to 1,072 candidate trajectories. We separated each root \RLM{} turn (i.e. iteration) as a separate SFT sample consisting of an input (the full history) and output (the output the root LM gave at that step).

We then applied a filtering step to remove turns beyond the context limit of Qwen3-8B (we approximated this as 100k characters), and also applied an extra programmatic correction step to fix small template mistakes in \RLM usage (e.g. outputting final answers, calling the REPL, etc.). 
To elaborate, we noticed that trajectories generated by Qwen3-Coder-480B-A35B had noticeable mistakes in following the \RLM{} instructions, which hurt the performance of the distilled RLM-Qwen3-8B. For example, it would often mix FINAL(answer) with FINAL(variable in REPL). We added an extra programmatic fixing step to look for common templated mistakes and patch them, leading to much better performance in the final \textbf{RLM-Qwen3-8B}. In total, 16\% of turns cleaned incorrectly used FINAL answers, and 13\% of turns incorrectly called a variable from the REPL (i.e. FINAL\_VAR) as a final answer. In Figure~\ref{fig:training-stats}, we show pre- and post-filtering statistics for our training trajectories.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=\textwidth]{figures/dataset_stats.png}
    \caption{We plot statistics for the \RLM{} trajectories on LongBenchPro that were collected and filtered to train \textbf{RLM-Qwen3-8B}. The left plots show the unfiltered trajectories, and right plots show the post-filtering trajectories.}
    \label{fig:training-stats}
\end{figure}


We used the \texttt{prime-rl} library~\citep{primeintellect2025prime-rl} for fine-tuning. We used a batch size of 64 for 300 training steps, training for 48 H100 hours. While this exceedingly simple training recipe was able to demonstrate substantial gains for our 8B model, we call on future work to investigate training native RLMs much more thoroughly. We expect that doing so at much larger scales in terms of model size, number and variety of examples, and number of (ideally on-policy and online) rollouts will be necessary to maximize the potential of RLMs.


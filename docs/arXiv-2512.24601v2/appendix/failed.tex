Drawing inspiration from ~\citet{redmon2018yolov3incrementalimprovement}, we try to be descriptive about what tricks, quirks, and other relevant things failed and succeeded in a concise manner. Some observations are based on longer supplementary experiments, while others are based on small samples of results.

\textbf{Using the exact same \RLM{} system prompt across all models can be problematic.} We originally wrote the \RLM{} system prompt with in context examples for GPT-5, and tried to use the same system prompt for Qwen3-Coder, but found that it led to different, undesirable behavior in the trajectory. We had to add a small sentence to the \RLM{} system prompt for Qwen3-Coder to prevent it from using too many recursive sub-calls.

\textbf{Models without sufficient coding capabilities struggle as \RLM{}s.} Our instantiation of \RLM{}s relies on the ability to reason through and deal with the context in a REPL environment. We found from small scale experiments that smaller models like Qwen3-8B~\citep{yang2025qwen3technicalreport} struggled without sufficient coding abilities.

\textbf{Thinking models without sufficient output tokens struggle as \RLM{}s.} In addition to \texttt{Qwen3-Coder-480B-A35B-Instruct}, we also tried experimenting with \texttt{Qwen3-235B-A22B} as the \RLM{}. While we found positive results across the board from the base model (e.g. on OOLONG~\citep{bertsch2025oolongevaluatinglongcontext}, performance jumped from $~30\%$ to $~38\%$), the smaller gap compared to the evaluated models in the main experiments (Table~\ref{tab:main}) are due to multiple trajectories running out of output tokens while producing outputs due to thinking tokens exceeding the maximum output token length of an individual LM call. 

\textbf{\RLM{}s without asynchronous LM calls are slow.} We implemented all sub-LM queries naively as blocking / sequential calls, which caused our \RLM{} experiments to be slow, especially compared to just the base model. We are confident that this can be resolved with a robust implementation.

\textbf{Depending on the model, distinguishing between a final answer and a thought is brittle for \RLM{}s.} The current strategy for distinguishing between a ``next turn" and a final answer for the \RLM{} is to have it wrap its answer in FINAL() or FINAL\_VAR() tags. Similar to intuition about structured outputs degrading performance, we also found the model to make strange decisions (e.g. it outputs its plan as a final answer). We added minor safeguards, but we also believe this issue should be avoided altogether in the future when models are trained as \RLM{}s.

